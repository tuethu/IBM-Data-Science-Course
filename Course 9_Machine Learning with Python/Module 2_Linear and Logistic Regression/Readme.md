# Module 2 Summary and Highlights
Congratulations! You have completed this lesson. At this point in the course, you know: 

## LINEAR REGRESSION

- Regression models relationships between a continuous target variable and explanatory features, covering simple and multiple regression types.

- Simple regression uses a single independent variable to estimate a dependent variable, while multiple regression involves more than one independent variable.

- Regression is widely applicable, from forecasting sales and estimating maintenance costs to predicting rainfall and disease spread.

- In simple linear regression, a best-fit line minimizes errors, measured by Mean Squared Error (MSE); this approach is known as Ordinary Least Squares (OLS).

- OLS regression is easy to interpret but sensitive to outliers, which can impact accuracy.

- Multiple linear regression extends simple linear regression by using multiple variables to predict outcomes and analyze variable relationships.

- Adding too many variables can lead to overfitting, so careful variable selection is necessary to build a balanced model.

- Nonlinear regression models complex relationships using polynomial, exponential, or logarithmic functions when data does not fit a straight line.

## POLYNOMIAL REGRESSION
- Polynomial regression can fit data but may overfit by capturing random noise rather than the underlying patterns.
  
- A researcher wants to model a non-linear growth in CO2 emissions as engine size increases, showing a smooth curve. Which regression technique is suitable in this scenario?<br/>
Polynomial regression. <br/>
For modeling a non-linear single-variable relationship, polynomial regression is the right choice.


## LOGISTIC REGRESSION

- Logistic regression is a probability predictor and binary classifier, suitable for binary targets and assessing feature impact.

- Logistic regression minimizes errors using log-loss and optimizes with gradient descent or stochastic gradient descent for efficiency.

- Gradient descent is an iterative process to minimize the cost function, which is crucial for training logistic regression models.

[Cheat Sheet: Linear and Logistic Regression](https://author-ide.skills.network/render?token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJtZF9pbnN0cnVjdGlvbnNfdXJsIjoiaHR0cHM6Ly9jZi1jb3Vyc2VzLWRhdGEuczMudXMuY2xvdWQtb2JqZWN0LXN0b3JhZ2UuYXBwZG9tYWluLmNsb3VkL3BWam5EcGZuOFN5ZVZuZmV1QTAzSlEvTTJMMyUyMENoZWF0c2hlZXQtVjIubWQ_dD0xNzQ2MTI3NDI5IiwidG9vbF90eXBlIjoiaW5zdHJ1Y3Rpb25hbC1sYWIiLCJhdGxhc19maWxlX2lkIjoyNTI0NjcsImFkbWluIjpmYWxzZSwiaWF0IjoxNzU3Njk3MjI3fQ.EZycV2hoZEIcAE6y0dy6h-Cf899Ixw3kxYpla3LKbgc)<br/>
[Simple Linear Regressio](https://github.com/tuethu/IBM-Data-Science-Course/blob/main/Course%209_Machine%20Learning%20with%20Python/Module%201_Introduction%20to%20Machine%20Learning/Lab_Simple%20Linear%20Regression.ipynb)<br/>
[Multiple Linear Regression](https://github.com/tuethu/IBM-Data-Science-Course/blob/main/Course%209_Machine%20Learning%20with%20Python/Module%201_Introduction%20to%20Machine%20Learning/Lab_Multiple%20Linear%20Regression.ipynb)<br/>
[Logistic Regression](https://github.com/tuethu/IBM-Data-Science-Course/blob/main/Course%209_Machine%20Learning%20with%20Python/Module%202_Linear%20and%20Logistic%20Regression/Lesson%202_Logistic%20Regression/Lab_Logistic%20Regression.ipynb)
